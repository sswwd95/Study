import tensorflow as tf
tf.set_random_seed(66) # 랜덤 값 고정해야 동일한 값이 출력된다

x_train = [1,2,3]
y_train =  [3,5,7]

W = tf.Variable(tf.random_normal([1]),name='weight')
# random_normal([1]) 정규분표에 따른 랜덤 값 [1]1개만 weight에 넣는다
b = tf.Variable(tf.random_normal([1]), name = 'bias')

# sess = tf.Session()
# sess.run(tf.global_variables_initializer()) 
# print(sess.run(W), sess.run(b))
# [0.06524777] [1.4264158]

hypothesis = x_train * W + b # -> y = wx + b

cost = tf.reduce_mean(tf.square(hypothesis - y_train)) 
# loss = mse (예측값에서 원래 결과값을 빼서 제곱하고 평균을 낸다)
# loss값을 최적화를 했다, gradient descent

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost) # cost를 집어넣고 최소화시키는 것을 훈련시키겠다 => 최적의 weight를 구한다

sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(2001):
    sess.run(train) # train을 실행시켜라
    if step % 20 ==0: # step을 20으로 나눈 나머지가 0이면 출력한다(2000 epoch돌면서 20번마다 출력)
        print(step, sess.run(cost), sess.run(W), sess.run(b))

# 결과 : weight는 2에 수렴하고 bias는 1에 수렴한다
'''
0 11.376854 [0.22876799] [1.4952775]
20 0.25022563 [1.4284163] [1.9631107]
40 0.13584478 [1.561582] [1.9646112]
60 0.12254739 [1.5923083] [1.9237307]
80 0.11129215 [1.612432] [1.8807428]
100 0.10107721 [1.6307381] [1.8393915]
120 0.09179988 [1.6481009] [1.7999469]
140 0.08337403 [1.6646397] [1.7623527]
160 0.07572166 [1.6804005] [1.7265248]
180 0.0687717 [1.6954204] [1.692381]
200 0.062459555 [1.7097346] [1.6598417]
220 0.056726765 [1.7233759] [1.6288317]
240 0.05152018 [1.7363763] [1.5992789]
260 0.04679146 [1.7487655] [1.5711151]
280 0.04249673 [1.7605727] [1.5442748]
300 0.038596284 [1.7718247] [1.518696]
320 0.03505375 [1.7825483] [1.494319]
340 0.03183632 [1.7927676] [1.4710876]
360 0.028914208 [1.8025069] [1.4489481]
380 0.0262604 [1.8117884] [1.4278493]
400 0.023850089 [1.8206335] [1.407742]
420 0.021661079 [1.8290632] [1.3885795]
440 0.019672872 [1.8370967] [1.3703175]
460 0.017867196 [1.8447527] [1.3529137]
480 0.016227258 [1.8520489] [1.3363278]
500 0.014737859 [1.8590021] [1.3205217]
520 0.013385173 [1.8656282] [1.3054585]
540 0.012156635 [1.8719432] [1.2911032]
560 0.011040834 [1.8779614] [1.2774227]
580 0.010027523 [1.8836967] [1.264385]
600 0.009107161 [1.8891624] [1.2519599]
620 0.00827124 [1.8943713] [1.2401189]
640 0.0075121033 [1.8993355] [1.2288343]
660 0.0068226103 [1.9040662] [1.21808]
680 0.0061964137 [1.9085747] [1.207831]
700 0.005627686 [1.9128714] [1.1980637]
720 0.005111159 [1.9169661] [1.1887556]
740 0.004642038 [1.9208684] [1.1798848]
760 0.004215974 [1.9245872] [1.1714308]
780 0.0038290105 [1.9281313] [1.1633744]
800 0.0034775827 [1.9315089] [1.1556964]
820 0.0031583782 [1.9347278] [1.1483792]
840 0.002868505 [1.9377952] [1.1414062]
860 0.0026052205 [1.9407187] [1.1347606]
880 0.0023660944 [1.9435047] [1.1284273]
900 0.0021489458 [1.9461597] [1.1223917]
920 0.001951709 [1.9486899] [1.1166399]
940 0.0017725708 [1.9511013] [1.1111583]
960 0.0016098769 [1.9533994] [1.1059341]
980 0.0014621211 [1.9555894] [1.1009557]
1000 0.0013279129 [1.9576765] [1.0962112]
1020 0.001206047 [1.9596653] [1.09169]
1040 0.0010953521 [1.9615611] [1.0873809]
1060 0.0009948144 [1.9633675] [1.0832744]
1080 0.0009035037 [1.965089] [1.079361]
1100 0.00082057266 [1.9667298] [1.0756311]
1120 0.00074525335 [1.9682934] [1.0720767]
1140 0.0006768556 [1.9697835] [1.0686892]
1160 0.0006147364 [1.9712036] [1.0654612]
1180 0.00055831386 [1.972557] [1.0623847]
1200 0.0005070635 [1.9738466] [1.0594529]
1220 0.00046053086 [1.9750756] [1.0566589]
1240 0.00041826023 [1.976247] [1.0539962]
1260 0.0003798761 [1.9773633] [1.0514586]
1280 0.0003450062 [1.9784269] [1.0490403]
1300 0.0003133389 [1.9794409] [1.0467358]
1320 0.00028457757 [1.9804074] [1.0445392]
1340 0.0002584597 [1.9813279] [1.042446]
1360 0.00023472484 [1.9822056] [1.0404505]
1380 0.00021318249 [1.9830421] [1.0385493]
1400 0.00019361461 [1.983839] [1.0367376]
1420 0.00017584577 [1.9845985] [1.0350112]
1440 0.00015970746 [1.9853224] [1.0333657]
1460 0.00014504963 [1.9860121] [1.0317976]
1480 0.0001317349 [1.9866694] [1.0303034]
1500 0.00011964602 [1.987296] [1.0288792]
1520 0.00010866277 [1.9878931] [1.027522]
1540 9.868818e-05 [1.988462] [1.0262285]
1560 8.9630485e-05 [1.9890041] [1.0249959]
1580 8.140441e-05 [1.9895209] [1.0238212]
1600 7.393402e-05 [1.9900134] [1.0227019]
1620 6.7150315e-05 [1.9904827] [1.021635]
1640 6.098339e-05 [1.99093] [1.0206182]
1660 5.5386172e-05 [1.9913564] [1.019649]
1680 5.030351e-05 [1.9917625] [1.0187256]
1700 4.568526e-05 [1.9921497] [1.0178455]
1720 4.1491487e-05 [1.9925187] [1.0170069]
1740 3.76838e-05 [1.9928702] [1.0162076]
1760 3.422509e-05 [1.9932052] [1.015446]
1780 3.1083884e-05 [1.9935246] [1.0147201]
1800 2.8232278e-05 [1.9938289] [1.0140283]
1820 2.563958e-05 [1.9941189] [1.0133691]
1840 2.328677e-05 [1.9943954] [1.0127407]
1860 2.1148328e-05 [1.9946587] [1.0121421]
1880 1.9209547e-05 [1.9949096] [1.0115715]
1900 1.7445474e-05 [1.9951489] [1.0110278]
1920 1.5844156e-05 [1.9953768] [1.0105095]
1940 1.4392031e-05 [1.9955939] [1.010016]
1960 1.3069816e-05 [1.9958011] [1.0095452]
1980 1.187079e-05 [1.9959984] [1.0090965]
2000 1.0781078e-05 [1.9961864] [1.0086691]
'''

# mse가 최소가 된다 = cost가 최소가 된다